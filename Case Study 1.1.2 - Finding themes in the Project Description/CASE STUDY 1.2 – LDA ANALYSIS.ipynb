{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_url = 'https://www.eecs.mit.edu/people/faculty-advisors'\n",
    "arXiv_format = 'arxiv.org/find/{}/1/au:+{}_{}/0/1/0/all/0/1' # arxiv.org/find/(subject)/1/au:+(lastname)_(initial)/0/1/0/all/0/1\n",
    "search_url_format = 'https://arxiv.org/search/?query=\"{}\"&searchtype=author'\n",
    "subjects = {'Computer Science': 'Computer Science', \n",
    "            'Electrical Engineering': 'Electrical Engineering and Systems Science',\n",
    "            'Physics': 'Physics'}\n",
    "all_papers_columns = ['Name', 'Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Sraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get Facultys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFacultyNames():\n",
    "    faculty_page = get(faculty_url)\n",
    "    faculty_page_content = BeautifulSoup(faculty_page.content, 'html.parser')\n",
    "    names_cont = faculty_page_content.select('div.views-field-title span.card-title a')\n",
    "    names = [name_cont.contents[0] for name_cont in names_cont]\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = getFacultyNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeArXiV(names):\n",
    "    papers = list()\n",
    "    for name in names:\n",
    "        search_url = search_url_format.format(name.replace(' ', '+'))\n",
    "        papers_author = get(search_url)\n",
    "        papers_author_content = BeautifulSoup(papers_author.content, 'html.parser')\n",
    "        papers_author_body = papers_author_content.body\n",
    "        results = papers_author_body.find_all(\"li\", class_=\"arxiv-result\")\n",
    "        abstracts = [result.find(\"span\", class_=\"abstract-full\") for result in results]\n",
    "        \n",
    "        abstracts_content = [abstract.a.unwrap() for abstract in abstracts]\n",
    "        abstracts_content = [abstract.contents[0] for abstract in abstracts]\n",
    "\n",
    "        if abstracts_content:\n",
    "            papers = papers + abstracts_content\n",
    "        \n",
    "    return papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = scrapeArXiV(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning_and_count(s):\n",
    "    s_lower = s.lower()\n",
    "    \n",
    "    cleaning_set = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(s_lower)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    word_dict = dict(collections.Counter(tokens))\n",
    "    for key in cleaning_set:\n",
    "        word_dict.pop(key, None)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_word_dict = [word_cleaning_and_count(paper) for paper in papers]\n",
    "dup_keys = []\n",
    "for i in range(len(papers_word_dict)):\n",
    "    dup_keys = dup_keys + list(papers_word_dict[i].keys())\n",
    "\n",
    "vocab = list(collections.Counter(dup_keys).keys())\n",
    "lookup_table = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/names', 'w') as fout:\n",
    "    json.dump(names, fout)\n",
    "with open('data/papers', 'w') as fout:\n",
    "    json.dump(papers, fout)\n",
    "with open('data/papers_word_dict', 'w') as fout:\n",
    "    json.dump(papers_word_dict, fout)\n",
    "with open('data/vocab', 'w') as fout:\n",
    "    json.dump(vocab, fout)\n",
    "with open('data/lookup_table', 'w') as fout:\n",
    "    json.dump(lookup_table, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/names', 'r') as json_file:\n",
    "    names = json.load(json_file)\n",
    "with open('data/papers', 'r') as json_file:\n",
    "    papers = json.load(json_file)\n",
    "with open('data/papers_word_dict', 'r') as json_file:\n",
    "    papers_word_dict = json.load(json_file)\n",
    "with open('data/vocab', 'r') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "with open('data/lookup_table', 'r') as json_file:\n",
    "    lookup_table = json.load(json_file)\n",
    "    \n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\data_science\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "problem n algorithm show time graph number algorithms also results\n",
      "Topic 1:\n",
      "quantum optical magnetic spin properties states graphene field material new\n",
      "Topic 2:\n",
      "data model network learning networks using neural new performance models\n",
      "Topic 3:\n",
      "channel capacity coding csi transmitter receiver cost scheme also wireless\n",
      "Topic 4:\n",
      "games game proof group groups prove paper equilibrium equilibria players\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "doc_vecs = []\n",
    "for paper in papers_word_dict: \n",
    "    doc_vec = [0 for _ in range(vocab_size)]\n",
    "    for token, occurs in paper.items(): \n",
    "        doc_vec[lookup_table[token]] = occurs\n",
    "    doc_vecs.append(doc_vec)\n",
    "\n",
    "# Run the LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, learning_method='online').fit(doc_vecs)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(' '.join([vocab[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, doc_vecs, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
