{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_url = 'https://www.eecs.mit.edu/people/faculty-advisors'\n",
    "arXiv_format = 'arxiv.org/find/{}/1/au:+{}_{}/0/1/0/all/0/1' # arxiv.org/find/(subject)/1/au:+(lastname)_(initial)/0/1/0/all/0/1\n",
    "search_url_format = 'https://arxiv.org/search/?query=\"{}\"&searchtype=author'\n",
    "subjects = {'Computer Science': 'Computer Science', \n",
    "            'Electrical Engineering': 'Electrical Engineering and Systems Science',\n",
    "            'Physics': 'Physics'}\n",
    "all_papers_columns = ['Name', 'Abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Sraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get Facultys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFacultyNames():\n",
    "    faculty_page = get(faculty_url)\n",
    "    faculty_page_content = BeautifulSoup(faculty_page.content, 'html.parser')\n",
    "    names_cont = faculty_page_content.select('div.views-field-title span.card-title a')\n",
    "    names = [name_cont.contents[0] for name_cont in names_cont]\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = getFacultyNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scrape Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeArXiV(names):\n",
    "    papers = list()\n",
    "    for name in names:\n",
    "        search_url = search_url_format.format(name.replace(' ', '+'))\n",
    "        papers_author = get(search_url)\n",
    "        papers_author_content = BeautifulSoup(papers_author.content, 'html.parser')\n",
    "        papers_author_body = papers_author_content.body\n",
    "        results = papers_author_body.find_all(\"li\", class_=\"arxiv-result\")\n",
    "        abstracts = [result.find(\"span\", class_=\"abstract-full\") for result in results]\n",
    "        \n",
    "        abstracts_content = [abstract.a.unwrap() for abstract in abstracts]\n",
    "        abstracts_content = [abstract.contents[0] for abstract in abstracts]\n",
    "\n",
    "        if abstracts_content:\n",
    "            papers = papers + abstracts_content\n",
    "        \n",
    "    return papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = scrapeArXiV(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cleaning_and_count(s):\n",
    "    s_lower = s.lower()\n",
    "    \n",
    "    cleaning_set = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(s_lower)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    word_dict = dict(collections.Counter(tokens))\n",
    "    for key in cleaning_set:\n",
    "        word_dict.pop(key, None)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_word_dict = [word_cleaning_and_count(paper) for paper in papers]\n",
    "dup_keys = []\n",
    "for i in range(len(papers_word_dict)):\n",
    "    dup_keys = dup_keys + list(papers_word_dict[i].keys())\n",
    "\n",
    "vocab = list(collections.Counter(dup_keys).keys())\n",
    "vocab_size = len(vocab)\n",
    "lookup_table = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "motion mri time temporal model series robust markov deformations volumetric\n",
      "Topic 1:\n",
      "refer vector seven proposed maps matching objective algorithm initial suggest\n",
      "Topic 2:\n",
      "problem matching pursuit unknown algorithms sparsity three squares least msso\n",
      "Topic 3:\n",
      "substantial second examining commonly registration two fetal template advantage often\n",
      "Topic 4:\n",
      "motion temporal time mri series images model registration assumption small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\conda\\conda\\envs\\data_science\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "doc_vecs = []\n",
    "for paper in papers_word_dict: \n",
    "    doc_vec = [0 for _ in range(vocab_size)]\n",
    "    for token, occurs in paper.items(): \n",
    "        doc_vec[lookup_table[token]] = occurs\n",
    "    doc_vecs.append(doc_vec)\n",
    "\n",
    "# Run the LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=200, learning_method='online', learning_offset=50.,random_state=0).fit(doc_vecs)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(' '.join([vocab[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, doc_vecs, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
